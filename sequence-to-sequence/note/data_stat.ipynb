{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import collections\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_dir):\n",
    "  start_time =time.time()\n",
    "  f = open(file_dir, 'r')\n",
    "  sentences = []\n",
    "  while True:\n",
    "    sentence = f.readline()\n",
    "    if not sentence:\n",
    "      break\n",
    "\n",
    "    sentence = sentence.strip().lower()\n",
    "    sentences.append(sentence)\n",
    "\n",
    "  f.close()\n",
    "  print(\"Loaded %d sentences from files, time %.2fs\" % (len(sentences), time.time() - start_time))\n",
    "  return sentences\n",
    "def char_statistics(sentences, counter):\n",
    "  for sentence in sentences:\n",
    "    counter.update(sentence)\n",
    "  return counter\n",
    "\n",
    "def filter_sentences_with_punct(sentences):\n",
    "  def filter_sentence(sentence):\n",
    "    sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r\"<u>|</u>\", r\"\", sentence)\n",
    "    #return re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "    return re.sub(r\"[^a-zA-Z0-9.,!?\\']+\", r\" \", sentence)\n",
    "  return [filter_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "def filter_sentences_without_punct(sentences):\n",
    "  def filter_sentence(sentence):\n",
    "    sentence = re.sub(r\"<u>|</u>\", r\"\", sentence)\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\']+\", r\" \", sentence)\n",
    "  return [filter_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "def tokenizer(sentence):\n",
    "  return nltk.word_tokenize(sentence)\n",
    "\n",
    "def build_vocab(sentences, max_words=None):\n",
    "  print_out(\"Buildding vocabulary...\")\n",
    "  word_count = Counter()\n",
    "  for sentence in sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "      word_count[word] += 1\n",
    "\n",
    "  print_out(\"The dataset has %d different words totally\" % len(word_count))\n",
    "  if not max_words:\n",
    "    max_words = len(word_count)\n",
    "  filter_out_words = len(word_count) - max_words\n",
    "\n",
    "  word_dict = word_count.most_common(max_words)\n",
    "  index2word = [\"<unk>\"] + [\"<s>\"] + [\"</s>\"] + [word[0] for word in word_dict]\n",
    "  word2index = dict([(word, idx) for idx, word in enumerate(index2word)])\n",
    "\n",
    "  print_out(\"%d words filtered out of the vocabulary and %d words in the vocabulary\" % (filter_out_words, max_words))\n",
    "  return index2word, word2index\n",
    "\n",
    "def build_vocab_with_nltk(sentences, max_words=None):\n",
    "  print_out(\"Buildding vocabulary...\")\n",
    "  word_count = Counter()\n",
    "  for sentence in sentences:\n",
    "    for word in tokenizer(sentence):\n",
    "      word_count[word] += 1\n",
    "\n",
    "  print_out(\"The dataset has %d different words totally\" % len(word_count))\n",
    "  if not max_words:\n",
    "    max_words = len(word_count)\n",
    "  filter_out_words = len(word_count) - max_words\n",
    "\n",
    "  word_dict = word_count.most_common(max_words)\n",
    "  index2word = [\"<unk>\"] + [\"<s>\"] + [\"</s>\"] + [word[0] for word in word_dict]\n",
    "  word2index = dict([(word, idx) for idx, word in enumerate(index2word)])\n",
    "\n",
    "  print_out(\"%d words filtered out of the vocabulary and %d words in the vocabulary\" % (filter_out_words, max_words))\n",
    "  return index2word, word2index\n",
    "\n",
    "def print_out(s, f=None, new_line=True):\n",
    "  if isinstance(s, bytes):\n",
    "    s = s.decode(\"utf-8\")\n",
    "\n",
    "  if f:\n",
    "    f.write(s.encode(\"utf-8\"))\n",
    "    if new_line:\n",
    "      f.write(b\"\\n\")\n",
    "\n",
    "  # stdout\n",
    "  out_s = s.encode(\"utf-8\")\n",
    "  if not isinstance(out_s, str):\n",
    "    out_s = out_s.decode(\"utf-8\")\n",
    "  print out_s,\n",
    "\n",
    "  if new_line:\n",
    "    sys.stdout.write(\"\\n\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "def load_data(file_dir):\n",
    "  print_out(\"Loading data files.\")\n",
    "  start_time =time.time()\n",
    "  f = open(file_dir, 'r')\n",
    "  sentences = []\n",
    "  while True:\n",
    "    sentence = f.readline()\n",
    "    if not sentence:\n",
    "      break\n",
    "\n",
    "    sentence = sentence.strip().lower()\n",
    "    sentences.append(sentence)\n",
    "\n",
    "  f.close()\n",
    "  print_out(\"Loaded %d sentences from files, time %.2fs\" % (len(sentences), time.time() - start_time))\n",
    "  return sentences\n",
    "\n",
    "def filter_sentences(sentences, whitelist):\n",
    "   def filter_sentence(sentence, whitelist):\n",
    "     return \"\".join([ch for ch in sentence if ch in whitelist])\n",
    "\n",
    "   return [filter_sentence(sentence, whitelist) for sentence in sentences] \n",
    "\n",
    "def build_worddict_test(sentences):\n",
    "  word_count = Counter()\n",
    "  for sentence in sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "      word_count[word] += 1\n",
    "  return word_count\n",
    "\n",
    "def build_worddict_test_1(sentences):\n",
    "  word_count = Counter()\n",
    "  for sentence in sentences:\n",
    "    for word in tokenizer(sentence):\n",
    "      word_count[word] += 1\n",
    "  return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files.\n",
      " Loaded 274270 sentences from files, time 0.22s\n",
      " [('\\x93', 29), ('\\x97', 1053), (' ', 2640874), ('\\xa3', 8), ('$', 156), ('\\xab', 1), (',', 150146), ('0', 2188), ('\\xb3', 1), ('4', 713), ('\\xb7', 1), ('8', 545), ('<', 6683), ('\\xc7', 1), ('\\xd3', 1), ('\\xdf', 1), ('`', 57), ('d', 401732), ('\\xe7', 6), ('h', 645961), ('l', 458108), ('\\xef', 1), ('p', 158704), ('\\xf3', 8), ('t', 1058170), ('x', 13916), ('\\xfb', 2), ('|', 49), ('\\x8c', 6), ('\\x94', 25), ('#', 19), (\"'\", 190799), ('+', 30), ('/', 3488), ('3', 740), ('7', 551), (';', 1377), ('?', 101644), ('\\xc8', 4), ('\\xd4', 2), ('[', 324), ('_', 123), ('\\xe0', 9), ('c', 233355), ('\\xe4', 26), ('g', 261967), ('\\xe8', 15), ('k', 159093), ('o', 1005373), ('s', 633793), ('w', 308988), ('{', 9), ('\\xfc', 2), ('\\x85', 84), ('\\x91', 17), ('\"', 8909), ('\\xa5', 1), ('&', 238), ('*', 1054), ('\\xad', 2), ('.', 442255), ('2', 1060), ('6', 521), ('\\xb9', 11), (':', 1551), ('>', 6685), ('\\xc9', 2), ('\\xd5', 1), ('^', 2), ('\\xe1', 5), ('b', 171482), ('f', 186886), ('\\xe9', 73), ('j', 30374), ('\\xed', 94), ('n', 732245), ('\\xf1', 8), ('r', 574546), ('v', 109099), ('\\xf9', 2), ('z', 8769), ('~', 24), ('\\x82', 3), ('\\t', 801), ('\\x8a', 1), ('\\x92', 1240), ('\\x96', 284), ('!', 32921), ('%', 66), (')', 51), ('-', 66162), ('1', 1695), ('\\xb2', 1), ('5', 840), ('9', 700), ('=', 11), ('\\xd2', 1), (']', 466), ('a', 837190), ('\\xe2', 3), ('e', 1282154), ('i', 797454), ('\\xea', 6), ('m', 304998), ('q', 6111), ('u', 436147), ('y', 383043), ('\\xfa', 1), ('}', 3)]\n",
      "Loading data files.\n",
      " Loaded 274270 sentences from files, time 0.21s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data(\"train.txt\")\n",
    "counter = Counter()\n",
    "char_counter = char_statistics(train_data, counter)\n",
    "print char_counter.items()\n",
    "chat_data = load_data(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chat_data_1 = filter_sentences_with_punct(chat_data)\n",
    "chat_data_2 = filter_sentences_without_punct(chat_data)\n",
    "chat_data_3 = filter_sentences(chat_data, '0123456789abcdefghijklmnopqrstuvwxyz\\' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buildding vocabulary...\n",
      " The dataset has 67370 different words totally\n",
      " 0 words filtered out of the vocabulary and 67370 words in the vocabulary\n",
      " Buildding vocabulary...\n",
      " The dataset has 49303 different words totally\n",
      " 0 words filtered out of the vocabulary and 49303 words in the vocabulary\n",
      " Buildding vocabulary...\n",
      " The dataset has 52088 different words totally\n",
      " 0 words filtered out of the vocabulary and 52088 words in the vocabulary\n",
      " Buildding vocabulary...\n",
      " The dataset has 47860 different words totally\n",
      " 0 words filtered out of the vocabulary and 47860 words in the vocabulary\n",
      " Buildding vocabulary...\n",
      " The dataset has 64860 different words totally\n",
      " 0 words filtered out of the vocabulary and 64860 words in the vocabulary\n",
      " Buildding vocabulary...\n",
      " The dataset has 60039 different words totally\n",
      " 0 words filtered out of the vocabulary and 60039 words in the vocabulary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_11 = build_vocab(chat_data_1)\n",
    "vocab_12 = build_vocab_with_nltk(chat_data_1)\n",
    "vocab_21 = build_vocab(chat_data_2)\n",
    "vocab_22 = build_vocab_with_nltk(chat_data_2)\n",
    "vocab_31 = build_vocab(chat_data_3)\n",
    "vocab_32 = build_vocab_with_nltk(chat_data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict_11 = build_worddict_test(chat_data_1)\n",
    "word_dict_12 = build_worddict_test_1(chat_data_1)\n",
    "word_dict_21 = build_worddict_test(chat_data_2)\n",
    "word_dict_22 = build_worddict_test_1(chat_data_2)\n",
    "word_dict_31 = build_worddict_test(chat_data_3)\n",
    "word_dict_32 = build_worddict_test_1(chat_data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_item_11 = word_dict_11.items()\n",
    "word_item_12 = word_dict_12.items()\n",
    "word_item_21 = word_dict_21.items()\n",
    "word_item_22 = word_dict_22.items()\n",
    "word_item_31 = word_dict_31.items()\n",
    "word_item_32 = word_dict_32.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list_11 = word_dict_11.most_common(50000)\n",
    "word_list_12 = word_dict_12.most_common(50000)\n",
    "word_list_21 = word_dict_21.most_common(50000)\n",
    "word_list_22 = word_dict_22.most_common(50000)\n",
    "word_list_31 = word_dict_31.most_common(50000)\n",
    "word_list_32 = word_dict_32.most_common(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nunnery', 1), ('woods', 84), ('clotted', 2), ('spiders', 5), ('hanging', 145), ('woody', 30), ('localized', 2), ('spidery', 1), ('sevens', 2), ('disobeying', 1)]\n"
     ]
    }
   ],
   "source": [
    "print word_item_12[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lungs', 28), ('dignity', 28), ('suffered', 28), ('hotels', 28), ('terrance', 28), ('awkward', 28), ('gag', 28), ('savings', 28), ('environment', 28), ('crashed', 28), ('underneath', 28), ('ammo', 28), ('nails', 28), ('calvin', 28), ('unlike', 28), ('batteries', 28), ('remembering', 28), ('yer', 28), ('sophie', 28), ('.with', 28), ('frightening', 28), ('approve', 28), ('haven', 28), ('lazy', 28), ('mayo', 28), ('bravo', 28), ('assistance', 28), ('marked', 28), ('leather', 28), ('lip', 28), ('presents', 28), ('differently', 28), ('panties', 28), ('ministry', 28), ('parent', 28), ('investigating', 28), ('gino', 28), ('fink', 28), ('follows', 28), ('expedition', 28), ('negro', 28), ('gather', 28), ('fortress', 28), ('arizona', 28), ('exclusive', 28), ('relieved', 28), ('wichita', 28), ('sittin', 28), ('debate', 28), ('buffy', 28)]\n"
     ]
    }
   ],
   "source": [
    "print word_list_12[4950:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
